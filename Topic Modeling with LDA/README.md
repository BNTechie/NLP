# Topic-Modeling

Topic modeling is a type of statistical modeling used to discover the abstract topics that occur in a collection of documents. One common approach to topic modeling is Latent Dirichlet Allocation (LDA). Below, I'll demonstrate how to perform topic modeling using LDA with Python's gensim library on a sample dataset.

A worked out example in [Jupyter notebook](https://github.com/BhadraNivedita/Topic-Modeling-in-Python/blob/main/%20Topic%20modeling%20using%20LDA%20to%20classify%20subjects%20in%20a%20collection%20of%20documents.ipynb)


# Topic Modeling with Latent Dirichlet Allocation (LDA)

## Introduction

Latent Dirichlet Allocation (LDA) is a generative probabilistic model used in natural language processing and machine learning for discovering abstract topics within a collection of documents. It's one of the most popular methods for topic modeling, allowing for the automatic classification of text into different topics based on the words present in the text.

### Key Concepts

1. **Documents and Words**:
   - A document is a collection of words.
   - A corpus is a collection of documents.

2. **Topics**:
   - Topics are distributions over a fixed vocabulary.
   - Each topic is characterized by a distribution of words, meaning each word in the vocabulary has a probability of belonging to that topic.

3. **Assumptions**:
   - Each document is assumed to be generated by a mixture of topics.
   - Each word in a document is generated from one of the topics, chosen according to the document-specific distribution over topics.

### How LDA Works

LDA assumes the following generative process for each document $\(d\)$ in a corpus $\(D\)$:

1. **Choose the Number of Topics** $\(K\)$.
2. **For Each Topic**:
   - Choose a distribution over words (this distribution is often assumed to be a Dirichlet distribution).
3. **For Each Document** $\(d\)$:
   - Choose a distribution over topics (also assumed to be a Dirichlet distribution).
   - For each word in the document:
     - Choose a topic from the topic distribution.
     - Choose a word from the corresponding word distribution for that topic.

### Parameters and Variables

- **$\(\alpha\)$**: Hyperparameter for the Dirichlet prior on the per-document topic distributions. It influences the number of topics per document.
- **$\(\beta\)$**: Hyperparameter for the Dirichlet prior on the per-topic word distribution. It influences the number of words per topic.
- **$\(\theta_d\)$**: Topic distribution for document $\(d\)$.
- **$\(\phi_k\)$**: Word distribution for topic $\(k\)$.
- **$\(z_{di}\)$**: Topic for the $\(i\)$-th word in document $\(d\)$.
- **$\(w_{di}\)$**: The $\(i\)$-th word in document $\(d\)$.

### Applications of LDA

1. **Topic Discovery**: Automatically discovering the topics present in a large collection of documents.
2. **Document Classification**: Classifying documents into topics for better organization.
3. **Recommendation Systems**: Recommending documents or content based on the topics inferred from user interactions.
4. **Text Summarization**: Summarizing documents by identifying the main topics.



